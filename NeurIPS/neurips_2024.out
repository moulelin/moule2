\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Preliminary}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{On-Policy Knowledge Distillation}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Uncertainty in LLMs}{section.2}% 4
\BOOKMARK [1][-]{section.3}{Method}{}% 5
\BOOKMARK [2][-]{subsection.3.1}{Offline Semantic Entropy Estimation}{section.3}% 6
\BOOKMARK [2][-]{subsection.3.2}{Uncertainty-Calibrated On-Policy Distillation}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.3}{Overall Training Objective}{section.3}% 8
\BOOKMARK [1][-]{section.4}{Experiments}{}% 9
\BOOKMARK [2][-]{subsection.4.1}{SURE-Math Dataset}{section.4}% 10
\BOOKMARK [2][-]{subsection.4.2}{Experimental Setup}{section.4}% 11
\BOOKMARK [2][-]{subsection.4.3}{Evaluation}{section.4}% 12
\BOOKMARK [2][-]{subsection.4.4}{Main Results}{section.4}% 13
\BOOKMARK [2][-]{subsection.4.5}{Ablation Studies}{section.4}% 14
\BOOKMARK [2][-]{subsection.4.6}{Analysis}{section.4}% 15
\BOOKMARK [1][-]{section.5}{Related Work}{}% 16
\BOOKMARK [2][-]{subsection.5.1}{Knowledge Distillation for Language Models}{section.5}% 17
\BOOKMARK [2][-]{subsection.5.2}{Learning from Verification Feedback}{section.5}% 18
\BOOKMARK [2][-]{subsection.5.3}{Contrastive Learning for Language Models}{section.5}% 19
\BOOKMARK [1][-]{section.6}{Conclusion}{}% 20
\BOOKMARK [1][-]{appendix.A}{Implementation Details}{}% 21
\BOOKMARK [2][-]{subsection.A.1}{Distributed Training Architecture}{appendix.A}% 22
\BOOKMARK [2][-]{subsection.A.2}{Memory Optimization Techniques}{appendix.A}% 23
\BOOKMARK [2][-]{subsection.A.3}{Communication Backend Selection}{appendix.A}% 24
\BOOKMARK [1][-]{appendix.B}{Additional Experimental Results}{}% 25
\BOOKMARK [2][-]{subsection.B.1}{Hyperparameter Sensitivity}{appendix.B}% 26
\BOOKMARK [2][-]{subsection.B.2}{Evaluation Protocol Details}{appendix.B}% 27
\BOOKMARK [2][-]{subsection.B.3}{Computational Cost}{appendix.B}% 28
\BOOKMARK [1][-]{appendix.C}{Reproducibility}{}% 29
