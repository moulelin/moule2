\section{Introduction}
\label{sec:intro}
Large-scale models \citep{comanici2025gemini} with billions of parameters have demonstrated remarkable capabilities but are limited, as substantial computational costs, in deployment on resource-constrained edge devices, such as satellites \citep{jia2025satellite, tao2025adaptive} and autonomous vehicles \citep{zhao2025survey, feng2025survey}.
%
Knowledge Distillation (KD) \citep{mansourian2025comprehensive, fang2026knowledge} transfers capabilities from an advanced model (teacher model) with billions of parameters to a small model (student model) that satisfies the strict efficiency requirements of such environments.
%
However, the inherent architectural heterogeneity between the two models often induces a significant distribution shift \citep{zhao2026self} and causes a mismatch that hinders the student from effectively adapt the teacher's representations.
\par
To mitigate this discrepancy, Generalized On-policy Distillation \citep{agarwal2024policy} addresses this issue by training the student on samples drawn from its current distribution rather than a fixed training set by leveraging Group Relative Policy Optimization (GRPO).
%
However, GKD treats all student-generated samples uniformly and does not exploit the correctness labels or reference chain-of-thought solutions that are readily available in reasoning benchmarks.

We observe that reasoning benchmarks inherently provide three complementary supervision signals, including dense token-level teacher distributions,  ground-truth, and reference chain-of-thought (CoT) solutions, whereas no existing method leverages them jointly.
%
To address this limitation, we introduce \textbf{CORE} (Contrastive On-policy Reasoning Distillation). Specifically, an answer-aware distillation objective conditions the teacher on the reference CoT and the ground-truth label, which allows it to provide dense token-level supervision over the full set of student-generated responses. Simultaneously, a contrastive preference optimization objective pairs each incorrectly answered response with a verified correct counterpart, thereby explicitly penalizing erroneous reasoning trajectories by Direct Preference Optimization (DPO).
%
Furthermore, CORE introduces two principled mechanisms to focus learning on the most informative signals. First, a token-level adaptive weighting scheme assigns higher importance to positions where the teacher-student divergence is large, directing gradient updates toward the reasoning steps that matter most. Second, an accuracy-driven loss balancing mechanism dynamically adjusts the relative strength of the distillation and contrastive objectives according to the current batch accuracy, producing an explicit curriculum that transitions from contrastive correction to distillation refinement as the student improves.

We evaluate \textbf{CORE} on complex reasoning benchmarks across different model sizes.
%
Extensive experiments demonstrate that CORE significantly outperforms state-of-the-art distillation methods, achieving notable improvements in accuracy and convergence speed.
