\section{Experiments}
\label{sec:experiments}

\subsection{SURE-Math Dataset}
\label{sec:dataset}

We introduce \textbf{SURE-Math} (\textbf{S}emantic-\textbf{U}ncertainty \textbf{RE}asoning \textbf{Math}), a mathematical reasoning dataset in which every problem is annotated with the teacher's precomputed semantic entropy score.

We first collect 1{,}000 mathematics problems spanning middle-school to competition level, each paired with a verified ground-truth answer. Of these, 200 are reserved as a held-out test set (Section~\ref{sec:evaluation}); the remaining 800 are added to the training pool.

We further aggregate seed problems from public sources across four difficulty tiers.
\emph{Easy}: ScaleQuest-Math \citep{yuan2024scalequest} (15K).
\emph{Medium}: NuminaMath-CoT \citep{numinamath} (10K) and MATH \citep{hendrycks2021math} (12.5K).
\emph{Hard}: Omni-MATH \citep{gao2024omnimath} (4.4K) and OlympiadBench \citep{he2024olympiadbench} (5K).
\emph{Competition}: AIME 2024--2025, HMMT Feb \& Nov 2025, and AMO-Bench \citep{amobench}.

Starting from these seeds, we synthesize new problems with Qwen2.5-72B-Instruct using an Evol-Instruct \citep{xu2024wizardlm} style pipeline. Six evolution strategies are applied, each realized by a dedicated system prompt: \emph{harder} (increase reasoning steps or add constraints), \emph{rewrite} (change context and wording while preserving the underlying skill), \emph{algebraize} (replace concrete values with variables), \emph{apply} (embed the concept in a real-world scenario), \emph{compose} (combine with a different branch of mathematics), and \emph{competition} (transform into AMC/AIME/Olympiad style).
Each seed undergoes 1--5 rounds of iterative evolution depending on its difficulty tier: easy seeds receive 1 round, medium seeds 2 rounds, hard seeds 3 rounds, and competition seeds 5 rounds. In follow-up rounds, a different strategy (drawn from \emph{harder}, \emph{competition}, and \emph{compose}) is applied to the output of the previous round, progressively increasing difficulty. Strategy selection within each tier is weighted to favor \emph{harder} and \emph{competition} for higher-tier seeds.


For each of the 8{,}000 problems, we sample $N{=}8$ responses from Qwen3-8B \citep{qwen3} at temperature 0.7, extract the $\texttt{\textbackslash boxed\{\}}$ answer from each response, and compute the probability-weighted semantic entropy as described in Section~\ref{sec:offline_se}. The resulting per-problem semantic entropy score and the corresponding distillation weight $w_{\text{se}}$ are stored alongside each problem.

We reserve 200 of the 1{,}000 manually collected problems (which carry ground-truth labels) as a held-out test set. The remaining 800 curated problems and all 8{,}000 evolved problems form the training set.

\subsection{Experimental Setup}
\label{sec:setup}

\paragraph{Models.}
We evaluate three student--teacher pairs of increasing scale: Qwen3-1.7B/Qwen3-8B, Qwen3-4B/Qwen3-8B, and Qwen3-8B/Qwen3-30B \citep{qwen3}, all instruction-tuned variants. Semantic clustering for the offline SE computation uses a Qwen2.5-3B-Instruct judge for pairwise semantic equivalence.

\paragraph{Training details.}
We train with the on-policy distillation objective described in Eq.~\ref{eq:uopd_loss}. At each step, the student generates $K{=}4$ rollouts per prompt. The teacher provides its top-512 logit values at every token position. We use a learning rate of $3{\times}10^{-6}$ with a cosine schedule and 5\% warmup, AdamW with $(\beta_1, \beta_2) = (0.9, 0.95)$, gradient clipping at 1.0, and a global batch size of 128. Training runs for one epoch over 50{,}000 samples (with replacement from the 8{,}800 training problems). All models use bfloat16 precision and FlashAttention-2.

For each student--teacher pair, we compare UOPD against the following methods.
(1)~\textbf{Base}: the student model without any distillation (lower bound).
(2)~\textbf{Teacher}: the teacher model (upper bound).
(3)~\textbf{Standard OPD}: on-policy distillation with uniform weighting ($w_{\text{se}} = 1$ for all prompts), following \citep{agarwal2024policy}.
(4)~\textbf{GRPO}: Group Relative Policy Optimization, which trains the student using reward signals from correct and incorrect rollouts.
(5)~\textbf{Self-Distilled Reasoner}: on-policy distillation with ground-truth chain-of-thought injected into the teacher's system prompt \citep{zhao2026self}.

\subsection{Evaluation}
\label{sec:evaluation}

\paragraph{Benchmarks.}
We evaluate all methods on three public mathematical reasoning benchmarks of increasing difficulty, plus our held-out test set:
\textbf{MATH-500} \citep{hendrycks2021math}, 500 competition-level problems spanning seven subjects;
\textbf{AIME 2025}, 30 problems from the American Invitational Mathematics Examination (Parts I and II);
\textbf{AMO-Bench} \citep{amobench}, 50 IMO-level competition problems;
and our held-out \textbf{SURE-Math test set} (200 curated problems with ground-truth labels).

\paragraph{Metrics.}
We report pass@1 accuracy with greedy decoding (temperature 0) and avg@16 accuracy with 16 samples at temperature 1.2 and top-$p{=}0.95$.

\subsection{Main Results}
\label{sec:main_results}

Table~\ref{tab:main_results} presents the main comparison across all benchmarks.

\begin{table}[t]
\centering
\caption{Main results on mathematical reasoning benchmarks. We report pass@1 (greedy) and avg@16 accuracy (\%). Best student-sized results are \textbf{bolded}.}
\label{tab:main_results}
\vspace{0.5em}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l cc cc cc cc}
\toprule
 & \multicolumn{2}{c}{\textbf{MATH-500}} & \multicolumn{2}{c}{\textbf{AIME 2025}} & \multicolumn{2}{c}{\textbf{AMO-Bench}} & \multicolumn{2}{c}{\textbf{SURE-Math}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
\textbf{Method} & pass@1 & avg@16 & pass@1 & avg@16 & pass@1 & avg@16 & pass@1 & avg@16 \\
\midrule

% --- Group 1 ---
\rowcolor{gray!10}
\multicolumn{9}{c}{\textbf{Student:} Qwen3-1.7B \quad \textbf{Teacher:} Qwen3-8B} \\
Base (1.7B)             & --   & --   & --   & --   & --   & --   & --   & -- \\
Standard OPD            & --   & --   & --   & --   & --   & --   & --   & -- \\
GRPO                    & --   & --   & --   & --   & --   & --   & --   & -- \\
Self-Distilled Reasoner & --   & --   & --   & --   & --   & --   & --   & -- \\
\textbf{UOPD (Ours)}   & --   & --   & --   & --   & --   & --   & --   & -- \\

\midrule

% --- Group 2 ---
\rowcolor{gray!10}
\multicolumn{9}{c}{\textbf{Student:} Qwen3-4B \quad \textbf{Teacher:} Qwen3-8B} \\
Base (4B)               & --   & --   & --   & --   & --   & --   & --   & -- \\
Standard OPD            & --   & --   & --   & --   & --   & --   & --   & -- \\
GRPO                    & --   & --   & --   & --   & --   & --   & --   & -- \\
Self-Distilled Reasoner & --   & --   & --   & --   & --   & --   & --   & -- \\
\textbf{UOPD (Ours)}   & --   & --   & --   & --   & --   & --   & --   & -- \\

\midrule

% --- Group 3 ---
\rowcolor{gray!10}
\multicolumn{9}{c}{\textbf{Student:} Qwen3-8B \quad \textbf{Teacher:} Qwen3-30B} \\
Base (8B)               & --   & --   & --   & --   & --   & --   & --   & -- \\
Standard OPD            & --   & --   & --   & --   & --   & --   & --   & -- \\
GRPO                    & --   & --   & --   & --   & --   & --   & --   & -- \\
Self-Distilled Reasoner & --   & --   & --   & --   & --   & --   & --   & -- \\
\textbf{UOPD (Ours)}   & --   & --   & --   & --   & --   & --   & --   & -- \\

\bottomrule
\end{tabular}}
\end{table}
% TODO: Fill in results once all evaluations are complete.
% Available so far:
%   Qwen3-1.7B base:      AIME25 greedy=26.7%, avg@16=35.6%
%   UOPD (step 21):       AIME25 greedy=26.7%, avg@16=33.8%
%   UOPD (step 21):       HMMT25 greedy=20.0%
%   UOPD (step 21):       AMO-Bench greedy=0.0%

\subsection{Ablation Studies}
\label{sec:ablation}

\paragraph{Effect of uncertainty weighting.}
We compare three weighting strategies: (1)~uniform weighting ($w_{\text{se}} = 1$), which reduces to standard on-policy distillation; (2)~binary filtering, which discards all prompts with $H_{\text{sem}} > \tau$ for a threshold $\tau$; and (3)~soft weighting ($w_{\text{se}} = 1 - H_{\text{sem}}/\log N$), which is our default. Table~\ref{tab:ablation_weighting} reports the results.

\begin{table}[h]
\centering
\caption{Ablation on uncertainty weighting strategies.}
\label{tab:ablation_weighting}
\vspace{0.5em}
\small
\begin{tabular}{l cccc}
\toprule
\textbf{Weighting} & \textbf{MATH-500} & \textbf{AIME 2025} & \textbf{AMO-Bench} & \textbf{SURE-Math} \\
\midrule
Uniform ($w = 1$)                          & -- & -- & -- & -- \\
Binary filter ($\tau = 0.3$)               & -- & -- & -- & -- \\
Binary filter ($\tau = 0.5$)               & -- & -- & -- & -- \\
Soft weighting (UOPD)                      & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Number of SE samples $N$.}
The number of teacher rollouts $N$ used for semantic entropy estimation controls the resolution of the uncertainty estimate. We vary $N \in \{2, 4, 8, 16\}$ and measure downstream distillation performance.

\paragraph{Top-$k$ logit truncation.}
Transmitting the full vocabulary ($|V| = 151{,}936$) is memory-intensive. We compare $k \in \{128, 512, 2048\}$ and full-vocabulary distillation, measuring both accuracy and peak GPU memory.

\paragraph{Number of student rollouts $K$.}
We ablate $K \in \{1, 2, 4, 8\}$ on-policy rollouts per prompt to understand the trade-off between training diversity and computational cost.

\subsection{Analysis}
\label{sec:analysis}

\paragraph{Convergence speed.}
% TODO: Plot training loss / eval accuracy curves for UOPD vs Standard OPD vs GRPO.

\paragraph{Semantic entropy distribution.}
Figure~\ref{fig:se_distribution} shows the distribution of semantic entropy scores across SURE-Math. The majority of problems have low SE, indicating that the teacher is confident on most prompts. The long tail of high-SE problems represents cases where the teacher genuinely disagrees with itself, and these are precisely the samples that UOPD downweights.

% TODO: Insert SE distribution histogram
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.48\textwidth]{figures/se_distribution.pdf}
% \caption{Distribution of semantic entropy scores in SURE-Math. Most problems have low SE (confident teacher), while the tail captures genuinely uncertain prompts.}
% \label{fig:se_distribution}
% \end{figure}

\paragraph{Validating SE as a teacher error signal.}
A core assumption of UOPD is that high semantic entropy indicates problems the teacher is likely to answer incorrectly.
We verify this directly on the SURE-Math training set.
For each of the 16{,}330 training problems, we obtain ground-truth labels by generating solutions with Qwen2.5-72B-Instruct and then evaluate the teacher (Qwen3-8B) with greedy decoding on the same problems.
The teacher's predicted answer is compared against the 72B-verified ground truth using exact-match and symbolic equivalence checking.

Figure~\ref{fig:se_validation}(a) bins the training problems by their precomputed SE score and plots the teacher's accuracy within each bin.
The relationship is strikingly monotonic: for problems with near-zero SE ($< 0.1$), the teacher achieves 87\% accuracy, whereas for problems with SE $> 0.9$, accuracy drops to just 12\%.
Figure~\ref{fig:se_validation}(b) treats teacher error as a binary classification target and uses SE as the predictor, yielding an AUROC of \textbf{0.7949}.
This confirms that probability-weighted semantic entropy is a reliable proxy for teacher correctness, validating the use of $w_{\text{se}} = 1 - \bar{H}_{\text{sem}}$ as a distillation weight: samples where the teacher is most likely wrong receive the lowest weight, while confident and correct samples dominate the training signal.

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.52\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/se_vs_accuracy.png}
    \caption{Teacher accuracy (Qwen3-8B, greedy) vs.\ SE score. Accuracy decreases monotonically from 87\% to 12\% as SE increases.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.44\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/roc_curve.png}
    \caption{ROC curve using SE as a predictor of teacher error, achieving an AUROC of 0.7949.}
\end{subfigure}
\caption{Empirical validation that semantic entropy predicts teacher error. Ground-truth labels are obtained from Qwen2.5-72B-Instruct on the 16{,}330 SURE-Math training problems. High SE reliably identifies samples where the teacher answers incorrectly, justifying the uncertainty-based weighting in UOPD.}
\label{fig:se_validation}
\end{figure}

\paragraph{Qualitative examples.}
Table~\ref{tab:qualitative} shows representative examples where UOPD's uncertainty weighting helps. For low-SE prompts, the teacher provides consistent guidance and the student learns effectively. For high-SE prompts, the teacher produces contradictory answers; UOPD suppresses these samples, preventing the student from learning incorrect reasoning patterns.

% TODO: Add qualitative example table
