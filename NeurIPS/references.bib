% GKD Paper
@inproceedings{agarwal2024policy,
  title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{zhao2026self,
  title={Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models},
  author={Zhao, Siyan and Xie, Zhihui and Liu, Mengchen and Huang, Jing and Pang, Guan and Chen, Feiyu and Grover, Aditya},
  journal={arXiv preprint arXiv:2601.18734},
  year={2026}
}
@article{fang2026knowledge,
  title={Knowledge distillation and dataset distillation of large language models: Emerging trends, challenges, and future directions},
  author={Fang, Luyang and Yu, Xiaowei and Cai, Jiazhang and Chen, Yongkai and Wu, Shushan and Liu, Zhengliang and Yang, Zhenyuan and Lu, Haoran and Gong, Xilin and Liu, Yufang and others},
  journal={Artificial Intelligence Review},
  volume={59},
  number={1},
  pages={17},
  year={2026},
  publisher={Springer}
}
@article{mansourian2025comprehensive,
  title={A comprehensive survey on knowledge distillation},
  author={Mansourian, Amir M and Ahmadi, Rozhan and Ghafouri, Masoud and Babaei, Amir Mohammad and Golezani, Elaheh Badali and Ghamchi, Zeynab Yasamani and Ramezanian, Vida and Taherian, Alireza and Dinashi, Kimia and Miri, Amirali and others},
  journal={arXiv preprint arXiv:2503.12067},
  year={2025}
}
@article{feng2025survey,
  title={A survey of world models for autonomous driving},
  author={Feng, Tuo and Wang, Wenguan and Yang, Yi},
  journal={arXiv preprint arXiv:2501.11260},
  year={2025}
}
@article{zhao2025survey,
  title={A survey of autonomous driving from a deep learning perspective},
  author={Zhao, Jingyuan and Wu, Yuyan and Deng, Rui and Xu, Susu and Gao, Jinpeng and Burke, Andrew},
  journal={ACM Computing Surveys},
  volume={57},
  number={10},
  pages={1--60},
  year={2025},
  publisher={ACM New York, NY}
}
@article{tao2025adaptive,
  title={Adaptive multi-layer deployment for a digital-twin-empowered satellite-terrestrial integrated network},
  author={Tao, Yihong and Lei, Bo and Shi, Haoyang and Chen, Jingkai and Zhang, Xing},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={26},
  number={2},
  pages={246--259},
  year={2025},
  publisher={Springer}
}
@article{jia2025satellite,
  title={Satellite Computing Network Construction: Optimal Computing Node Deployment in Multi-Layer LEO Mega-Constellations},
  author={Jia, Xiao and Zhou, Di and Sheng, Min and Shi, Yan and Ji, Sijing and Li, Jiandong},
  journal={IEEE Transactions on Communications},
  volume={74},
  pages={1747--1761},
  year={2025},
  publisher={IEEE}
}
@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}
% Chain-of-Thought
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

% Knowledge Distillation
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

% DistilBERT
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

% Sequence-level KD
@article{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016}
}

% DPO
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

% GSM8K
@article{cobbe2021gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

% Add more references as needed
% TODO: Ray, vLLM, DeepSpeed, GRPO, etc.
