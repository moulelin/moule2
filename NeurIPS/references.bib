% GKD Paper
@inproceedings{agarwal2024policy,
  title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{zhao2026self,
  title={Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models},
  author={Zhao, Siyan and Xie, Zhihui and Liu, Mengchen and Huang, Jing and Pang, Guan and Chen, Feiyu and Grover, Aditya},
  journal={arXiv preprint arXiv:2601.18734},
  year={2026}
}
@article{fang2026knowledge,
  title={Knowledge distillation and dataset distillation of large language models: Emerging trends, challenges, and future directions},
  author={Fang, Luyang and Yu, Xiaowei and Cai, Jiazhang and Chen, Yongkai and Wu, Shushan and Liu, Zhengliang and Yang, Zhenyuan and Lu, Haoran and Gong, Xilin and Liu, Yufang and others},
  journal={Artificial Intelligence Review},
  volume={59},
  number={1},
  pages={17},
  year={2026},
  publisher={Springer}
}
@article{mansourian2025comprehensive,
  title={A comprehensive survey on knowledge distillation},
  author={Mansourian, Amir M and Ahmadi, Rozhan and Ghafouri, Masoud and Babaei, Amir Mohammad and Golezani, Elaheh Badali and Ghamchi, Zeynab Yasamani and Ramezanian, Vida and Taherian, Alireza and Dinashi, Kimia and Miri, Amirali and others},
  journal={arXiv preprint arXiv:2503.12067},
  year={2025}
}
@article{feng2025survey,
  title={A survey of world models for autonomous driving},
  author={Feng, Tuo and Wang, Wenguan and Yang, Yi},
  journal={arXiv preprint arXiv:2501.11260},
  year={2025}
}
@article{zhao2025survey,
  title={A survey of autonomous driving from a deep learning perspective},
  author={Zhao, Jingyuan and Wu, Yuyan and Deng, Rui and Xu, Susu and Gao, Jinpeng and Burke, Andrew},
  journal={ACM Computing Surveys},
  volume={57},
  number={10},
  pages={1--60},
  year={2025},
  publisher={ACM New York, NY}
}
@article{tao2025adaptive,
  title={Adaptive multi-layer deployment for a digital-twin-empowered satellite-terrestrial integrated network},
  author={Tao, Yihong and Lei, Bo and Shi, Haoyang and Chen, Jingkai and Zhang, Xing},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={26},
  number={2},
  pages={246--259},
  year={2025},
  publisher={Springer}
}
@article{jia2025satellite,
  title={Satellite Computing Network Construction: Optimal Computing Node Deployment in Multi-Layer LEO Mega-Constellations},
  author={Jia, Xiao and Zhou, Di and Sheng, Min and Shi, Yan and Ji, Sijing and Li, Jiandong},
  journal={IEEE Transactions on Communications},
  volume={74},
  pages={1747--1761},
  year={2025},
  publisher={IEEE}
}
@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}
% Chain-of-Thought
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

% Knowledge Distillation
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

% DistilBERT
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

% Sequence-level KD
@article{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016}
}

% DPO
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

% GSM8K
@article{cobbe2021gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

% Aleatoric vs Epistemic Uncertainty
@inproceedings{kendall2017uncertainties,
  title={What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

% Semantic Uncertainty
@inproceedings{kuhn2023semantic,
  title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation},
  author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

% Qwen3
@article{qwen3,
  title={Qwen3 Technical Report},
  author={Yang, An and Yang, Anfeng and Yang, Baosong and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

% ScaleQuest
@inproceedings{yuan2024scalequest,
  title={ScaleQuest: Scalable Data Synthesis for Mathematical Reasoning},
  author={Yuan, Zhengyang and Liu, Jiawei and Zi, Boyuan and Ning, Hao and Zheng, Kai and Chen, Minghao and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

% NuminaMath
@article{numinamath,
  title={NuminaMath-CoT: A Large-Scale Mathematical Reasoning Dataset with Chain-of-Thought Annotations},
  author={{AI-MO Team}},
  year={2024},
  note={\url{https://huggingface.co/datasets/AI-MO/NuminaMath-CoT}}
}

% MATH (Hendrycks)
@inproceedings{hendrycks2021math,
  title={Measuring Mathematical Problem Solving With the {MATH} Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

% Omni-MATH
@article{gao2024omnimath,
  title={Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models},
  author={Gao, Bofei and Song, Feifan and Yang, Zhe and Cai, Zefan and Miao, Yibo and Dong, Qingxiu and Li, Lei and Ma, Chenghao and Chen, Liang and Xu, Runxin and others},
  journal={arXiv preprint arXiv:2410.07985},
  year={2024}
}

% OlympiadBench
@article{he2024olympiadbench,
  title={OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}

% AMO-Bench
@article{amobench,
  title={AMO-Bench: Assessing Mathematical Olympiad Problem Solving for Large Language Models},
  author={{Meituan Longcat Team}},
  year={2025},
  note={\url{https://huggingface.co/datasets/meituan-longcat/AMO-Bench}}
}

% Evol-Instruct / WizardLM
@inproceedings{xu2024wizardlm,
  title={WizardLM: Empowering Large Language Models to Follow Complex Instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}
