\begin{thebibliography}{10}

\bibitem{comanici2025gemini}
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et~al.
\newblock Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.
\newblock {\em arXiv preprint arXiv:2507.06261}, 2025.

\bibitem{jia2025satellite}
Xiao Jia, Di~Zhou, Min Sheng, Yan Shi, Sijing Ji, and Jiandong Li.
\newblock Satellite computing network construction: Optimal computing node deployment in multi-layer leo mega-constellations.
\newblock {\em IEEE Transactions on Communications}, 74:1747--1761, 2025.

\bibitem{tao2025adaptive}
Yihong Tao, Bo~Lei, Haoyang Shi, Jingkai Chen, and Xing Zhang.
\newblock Adaptive multi-layer deployment for a digital-twin-empowered satellite-terrestrial integrated network.
\newblock {\em Frontiers of Information Technology \& Electronic Engineering}, 26(2):246--259, 2025.

\bibitem{zhao2025survey}
Jingyuan Zhao, Yuyan Wu, Rui Deng, Susu Xu, Jinpeng Gao, and Andrew Burke.
\newblock A survey of autonomous driving from a deep learning perspective.
\newblock {\em ACM Computing Surveys}, 57(10):1--60, 2025.

\bibitem{feng2025survey}
Tuo Feng, Wenguan Wang, and Yi~Yang.
\newblock A survey of world models for autonomous driving.
\newblock {\em arXiv preprint arXiv:2501.11260}, 2025.

\bibitem{mansourian2025comprehensive}
Amir~M Mansourian, Rozhan Ahmadi, Masoud Ghafouri, Amir~Mohammad Babaei, Elaheh~Badali Golezani, Zeynab~Yasamani Ghamchi, Vida Ramezanian, Alireza Taherian, Kimia Dinashi, Amirali Miri, et~al.
\newblock A comprehensive survey on knowledge distillation.
\newblock {\em arXiv preprint arXiv:2503.12067}, 2025.

\bibitem{fang2026knowledge}
Luyang Fang, Xiaowei Yu, Jiazhang Cai, Yongkai Chen, Shushan Wu, Zhengliang Liu, Zhenyuan Yang, Haoran Lu, Xilin Gong, Yufang Liu, et~al.
\newblock Knowledge distillation and dataset distillation of large language models: Emerging trends, challenges, and future directions.
\newblock {\em Artificial Intelligence Review}, 59(1):17, 2026.

\bibitem{zhao2026self}
Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, and Aditya Grover.
\newblock Self-distilled reasoner: On-policy self-distillation for large language models.
\newblock {\em arXiv preprint arXiv:2601.18734}, 2026.

\bibitem{agarwal2024policy}
Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem.
\newblock On-policy distillation of language models: Learning from self-generated mistakes.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2024.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{kim2016sequence}
Yoon Kim and Alexander~M Rush.
\newblock Sequence-level knowledge distillation.
\newblock {\em arXiv preprint arXiv:1606.07947}, 2016.

\bibitem{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\end{thebibliography}
