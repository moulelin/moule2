\begin{thebibliography}{10}

\bibitem{zhao2026self}
Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, and Aditya Grover.
\newblock Self-distilled reasoner: On-policy self-distillation for large language models.
\newblock {\em arXiv preprint arXiv:2601.18734}, 2026.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{agarwal2024policy}
Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem.
\newblock On-policy distillation of language models: Learning from self-generated mistakes.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2024.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824--24837, 2022.

\bibitem{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in bayesian deep learning for computer vision?
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{kuhn2023semantic}
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar.
\newblock Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{qwen3}
An~Yang, Anfeng Yang, Baosong Yang, et~al.
\newblock Qwen3 technical report.
\newblock {\em arXiv preprint arXiv:2505.09388}, 2025.

\bibitem{yuan2024scalequest}
Zhengyang Yuan, Jiawei Liu, Boyuan Zi, Hao Ning, Kai Zheng, Minghao Chen, et~al.
\newblock Scalequest: Scalable data synthesis for mathematical reasoning.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2025.

\bibitem{numinamath}
{AI-MO Team}.
\newblock Numinamath-cot: A large-scale mathematical reasoning dataset with chain-of-thought annotations.
\newblock 2024.
\newblock \url{https://huggingface.co/datasets/AI-MO/NuminaMath-CoT}.

\bibitem{hendrycks2021math}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the {MATH} dataset.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{gao2024omnimath}
Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et~al.
\newblock Omni-math: A universal olympiad level mathematic benchmark for large language models.
\newblock {\em arXiv preprint arXiv:2410.07985}, 2024.

\bibitem{he2024olympiadbench}
Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen~Leng Thai, Junhao Shen, Jinyi Hu, Xu~Han, Yujie Huang, Yuxiang Zhang, et~al.
\newblock Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems.
\newblock {\em arXiv preprint arXiv:2402.14008}, 2024.

\bibitem{amobench}
{Meituan Longcat Team}.
\newblock Amo-bench: Assessing mathematical olympiad problem solving for large language models.
\newblock 2025.
\newblock \url{https://huggingface.co/datasets/meituan-longcat/AMO-Bench}.

\bibitem{xu2024wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex instructions.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2024.

\bibitem{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{kim2016sequence}
Yoon Kim and Alexander~M Rush.
\newblock Sequence-level knowledge distillation.
\newblock {\em arXiv preprint arXiv:1606.07947}, 2016.

\bibitem{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\end{thebibliography}
